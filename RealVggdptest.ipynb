{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealVggdptest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theolouvet/DatasetTest/blob/master/RealVggdptest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJCHgod8EbA8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "301457e4-921f-4525-f76c-cdd3933f00a6"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Normalize, Compose\n",
        "import tarfile\n",
        "\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "import io\n",
        "import cv2 \n",
        "import os\n",
        "import sys\n",
        "from google.colab import files\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from io import BytesIO\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAgjgkI1EhDB"
      },
      "source": [
        "def count_parameters(model):\n",
        "    #for parameter in model.parameters():\n",
        "    #    print(parameter)\n",
        "    #print ('nb of trainable parameters')\n",
        "    return (sum([p.numel() for p in model.parameters() if p.requires_grad]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps73cXw7Puhx"
      },
      "source": [
        "class Subset(torch.utils.data.Dataset):\n",
        "      def __init__(self, dataset, indices):\n",
        "          self.dataset = dataset\n",
        "          self.indices = indices\n",
        "      def __getitem__(self, idx):\n",
        "          return self.dataset[self.indices[idx]]\n",
        "      def __len__(self):\n",
        "          return len(self.indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwplB6uNRh03"
      },
      "source": [
        "def padding(img,expected_size):\n",
        "    desired_size = expected_size\n",
        "    delta_width = desired_size - img.size[0]\n",
        "    delta_height = desired_size - img.size[1]\n",
        "    pad_width = delta_width //2\n",
        "    pad_height = delta_height //2\n",
        "    padding = (pad_width,pad_height,delta_width-pad_width,delta_height-pad_height)\n",
        "    return ImageOps.expand(img, padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2eqL5dyEqU6"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, txt_path='/content/Output_seuillage/small_training_dataset.csv'\n",
        ", img_dir='/content/Output_seuillage/', transform=None, test=False):\n",
        "     # initialize variables such is path to csv file and images and transforms\n",
        "    df = pd.read_csv(txt_path)\n",
        "   \n",
        "    #self.categories = categories\n",
        "    self.img_names = df.ID.values\n",
        "    self.format_img = '.jpg'\n",
        "    self.species = df.species.values\n",
        "    self.txt_path = txt_path\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.to_tensor = ToTensor()\n",
        "    self.to_pil = ToPILImage()\n",
        "\n",
        "    self.get_image_selector = True if img_dir.__contains__('tar') else False\n",
        "    self.tf = tarfile.open(self.img_dir) if self.get_image_selector else None\n",
        "    self.transform_gt = transform if test else Compose(self.transform.transforms[:-1])  # omit noise of ground truth\n",
        "    \n",
        "    print(\"dataset initialise\")\n",
        "  def set_categories(self, cat):\n",
        "    self.categories = cat\n",
        "  def __len__(self):\n",
        "      # here you just need to return a single integer number as the length of your dataset, in your \n",
        "      #  case, number of images in your train folder or lines in csv file\n",
        "    return len(self.img_names)\n",
        "      #return 1;\n",
        "  def get_image_from_folder(self, name):\n",
        "        \"\"\"\n",
        "        gets a image by a name gathered from file list text file\n",
        "        :param name: name of targeted image\n",
        "        :return: a PIL image\n",
        "        \"\"\"\n",
        "        image_name = os.path.join(self.img_dir, str(name))\n",
        "        image_name =image_name + str(self.format_img)\n",
        "        image = Image.open(image_name)\n",
        "        #image = plt.imread(image_name)\n",
        "        #print(image.size())\n",
        "        newsize = 256 \n",
        "        resize = (224, 224)\n",
        "        image = padding(image, newsize)\n",
        "        image = image.resize(resize) \n",
        "        trans1 = transforms.ToTensor()\n",
        "      \n",
        "        return trans1(image)\n",
        "  def getTarget(self, index):\n",
        "    return self.categories.get(self.species[index])\n",
        "  def getNameSpecies(self, idDictionary):\n",
        "    return list(test.keys())[list(test.values()).index(idDictionary)]\n",
        "  def show(self, index):\n",
        "    trans = transforms.ToPILImage()\n",
        "    img = self.get_image_from_folder(self.img_names[index])\n",
        "    plt.imshow(trans(img))\n",
        "  def __getitem__(self, index): \n",
        "    if index == (self.__len__() - 1) and self.get_image_selector:  # close tarfile opened in __init__\n",
        "              self.tf.close()\n",
        "    img = self.get_image_from_folder(self.img_names[index])\n",
        "    #spc = self.species[index]\n",
        "    spc = self.getTarget(index)\n",
        "    \n",
        "    sample = (img, spc)\n",
        "    \n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J51CDeAcoAcu"
      },
      "source": [
        "def zip_extraction_mf(file_name, filename_test, path_csv, path_csv_test):\n",
        "  with ZipFile(file_name_test, 'r') as zip:\n",
        "    zip.extractall();\n",
        "\n",
        "  with ZipFile(file_name, 'r') as zip:\n",
        "    zip.extractall();\n",
        "    print('done')\n",
        "  csv = pd.read_csv(path_csv)\n",
        "  csv.columns = [c.replace(' ', '_') for c in csv.columns]\n",
        "\n",
        "  csv_test = pd.read_csv(path_csv_test)\n",
        "  csv_test.columns = [c.replace(' ', '_') for c in csv.columns]\n",
        "\n",
        "  return csv, csv_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XYjzzWmvLIN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A2zwOM3RXL2"
      },
      "source": [
        "class NetVgg16(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NetVgg16,self).__init__()\n",
        "    print(\"initialisation net VGG16\")\n",
        "    self.name = 'vgg'\n",
        "    #block1\n",
        "    self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(3, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU())\n",
        "    \n",
        "    self.conv2 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(64, 128, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(128, 128, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv3 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(128, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv4 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(256, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    self.mp5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ))  #torch.Size([64, 512, 6, 6])\n",
        "    \n",
        "  \n",
        "    D_in, H, D_out = 512*7*7, 1000, 30\n",
        "\n",
        "    self.fconnected = nn.Sequential(\n",
        "                      nn.Flatten(),\n",
        "                      torch.nn.Linear(D_in, H),\n",
        "                      torch.nn.ReLU(),\n",
        "                      torch.nn.Linear(H, D_out))\n",
        "                      #nn.Linear(512 * 7 * 7 , 100),\n",
        "                      #nn.ReLU())\n",
        "                      #torch.nn.Linear(1000, 50))\n",
        "\n",
        "    self.sm = nn.Softmax(dim = 1)\n",
        "    # Block 2\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    \n",
        "    x = self.conv3(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    bs = self.conv5(x).size()[0]\n",
        "    \n",
        "    x = self.conv5(x)\n",
        "\n",
        "    x = self.mp5(x)\n",
        "\n",
        "    x  = x.view(bs,-1)\n",
        "\n",
        "    x = self.fconnected(x)\n",
        "    \n",
        "    x = self.sm(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH9cRpOd6Acu"
      },
      "source": [
        "class mininet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(mininet,self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(1, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU())\n",
        "    D_in, H, D_out = 25088, 1000, 50\n",
        "    self.fconnected = nn.Sequential(\n",
        "                      nn.Flatten(),\n",
        "                      torch.nn.Linear(D_in, H),\n",
        "                      torch.nn.ReLU(),\n",
        "                      torch.nn.Linear(H, D_out))\n",
        "                      #nn.Linear(512 * 7 * 7 , 100),\n",
        "                      #nn.ReLU())\n",
        "                      #torch.nn.Linear(1000, 50))\n",
        "\n",
        "    self.sm = nn.Softmax(dim = 1)\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "    x = self.fconnected(x)\n",
        "    x = self.sm(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qSIFmuLRDNa"
      },
      "source": [
        "class NetVgg19(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NetVgg19,self).__init__()\n",
        "    print(\"initialisation net VGG19\")\n",
        "    self.name = 'vgg'\n",
        "    #block1\n",
        "    self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(3, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU())\n",
        "    \n",
        "    self.conv2 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(64, 128, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(128, 128, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv3 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(128, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv4 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(256, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    self.mp5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ))  #torch.Size([64, 512, 6, 6])\n",
        "    \n",
        "  \n",
        "    D_in, H, D_out = 25088, 50, 50\n",
        "\n",
        "    self.fconnected = nn.Sequential(\n",
        "                      nn.Flatten(),\n",
        "                      torch.nn.Linear(D_in, H),\n",
        "                      torch.nn.ReLU(),\n",
        "                      torch.nn.Linear(H, D_out))\n",
        "                      #nn.Linear(512 * 7 * 7 , 100),\n",
        "                      #nn.ReLU())\n",
        "                      #torch.nn.Linear(1000, 50))\n",
        "\n",
        "    self.sm = nn.Softmax(dim = 1)\n",
        "    # Block 2\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    \n",
        "    x = self.conv3(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    bs = self.conv5(x).size()[0]\n",
        "    \n",
        "    x = self.conv5(x)\n",
        "\n",
        "    x = self.mp5(x)\n",
        "\n",
        "    x  = x.view(bs,-1)\n",
        "\n",
        "    x = self.fconnected(x)\n",
        "    \n",
        "    x = self.sm(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1B4JLVgE7f3"
      },
      "source": [
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        #print(m)\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            #if m.bias:\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant_(m.weight, 1)\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal_(m.weight, std=1e-3)\n",
        "            #if m.bias:\n",
        "            init.constant_(m.bias, 0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDSxF8fQFA1y"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "file_name = \"training_dataset_50_90_pourcent.zip\"\n",
        "file_name_test = \"testing_dataset_50_90_pourcent.zip\"\n",
        "file_name = \"train_50_DL.zip\"\n",
        "file_name_test = \"test_50_DL.zip\"\n",
        "path_csv_test = '/content/test/test_dataset_50_90_pourcent.csv'\n",
        "path_csv_test = '/content/test/testing_Dataset_50_seuillage_200.csv'\n",
        "path_csv = '/content/train/train_dataset_50_90_pourcent.csv'\n",
        "path_csv= '/content/train/training_Dataset_50_seuillage_200.csv'\n",
        "\n",
        "\n",
        "file_name = \"/content/train_30_90_10_VeinSegm_256.zip\"\n",
        "file_name_test = \"/content/test_30_90_10_VeinSgem_256.zip\"\n",
        "\n",
        "path_csv_test = '/content/test/test_dataset_30_90_10.csv'\n",
        "path_csv = '/content/train/train_dataset_30_90_10.csv'\n",
        "\n",
        "\n",
        "file_name = \"/content/train.zip\"\n",
        "file_name_test = \"/content/test.zip\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Qt77jXFCkQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "975638ec-2fa9-4443-ecfe-d5cac209465a"
      },
      "source": [
        "csv, csv_test = zip_extraction_mf(file_name, file_name_test, path_csv, path_csv_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJSbr-IlEwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f11df25f-ef90-433a-8b6f-ce2cd0045c95"
      },
      "source": [
        "df = pd.read_csv(path_csv)\n",
        "categories = {}\n",
        "i = 0\n",
        "for row in df.species.values:\n",
        "  if row not in categories:\n",
        "    categories[row] = i\n",
        "    i = i + 1\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Populus nigra': 0, 'Viburnum tinus': 1, 'Rhus coriaria': 2, 'Phillyrea angustifolia': 3, 'Pittosporum tenuifolium': 4, 'Quercus ilex': 5, 'Pittosporum tobira': 6, 'Populus alba': 7, 'Betula pendula': 8, 'Hedera helix': 9, 'Crataegus monogyna': 10, 'Olea europaea': 11, 'Arbutus unedo': 12, 'Acer monspessulanum': 13, 'Daphne cneorum': 14, 'Juniperus oxycedrus': 15, 'Pistacia lentiscus': 16, 'Buxus sempervirens': 17, 'Buddleja davidii': 18, 'Ginkgo biloba': 19, 'Ulmus minor': 20, 'Celtis australis': 21, 'Euphorbia characias': 22, 'Carpinus betulus': 23, 'Acer campestre': 24, 'Ruscus aculeatus': 25, 'Punica granatum': 26, 'Cercis siliquastrum': 27, 'Cotinus coggygria': 28, 'Rhamnus alaternus': 29}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvYotOmQFMmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1d32a3d6-c5af-464c-c766-4b1f625bcc53"
      },
      "source": [
        "custom_transforms = Compose([ ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset= ImageDataset(txt_path=path_csv,\n",
        "                              img_dir='/content/train/',\n",
        "                              transform= custom_transforms,\n",
        "\n",
        "                              test=False)\n",
        "train_dataset.set_categories(categories)\n",
        "\n",
        "test_dataset= ImageDataset(txt_path=path_csv_test,\n",
        "                              img_dir='/content/test/',\n",
        "                              transform= custom_transforms,\n",
        "\n",
        "                              test=False)\n",
        "test_dataset.set_categories(categories)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset initialise\n",
            "dataset initialise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgyHgX1cFQut"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "#structure de pytorch facilitant l acces a une base de donnée\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik_9elQGFeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6f17885f-5596-4273-959c-87ccda33b157"
      },
      "source": [
        "'''\n",
        "net = NetVgg16()\n",
        "net = net.cuda()\n",
        "for batch_idx, (inputs,targets) in enumerate(trainloader):\n",
        "  break\n",
        "inputs = inputs.cuda()\n",
        "print(inputs.size())\n",
        "output = net(inputs)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnet = NetVgg16()\\nnet = net.cuda()\\nfor batch_idx, (inputs,targets) in enumerate(trainloader):\\n  break\\ninputs = inputs.cuda()\\nprint(inputs.size())\\noutput = net(inputs)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPaYTS_nVFhW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8630776a-fd29-46b8-c0f3-b15a967ddf60"
      },
      "source": [
        "'''image = Image.open('/content/test/10054.jpg')\n",
        "#image = plt.imread(image_name)\n",
        "#print(image.size())\n",
        "newsize = 256\n",
        "padding(image, newsize)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"image = Image.open('/content/test/10054.jpg')\\n#image = plt.imread(image_name)\\n#print(image.size())\\nnewsize = 256\\npadding(image, newsize)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9dZUnt9JgAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78b6ba90-deb5-4b8d-c744-589222e3c881"
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "#cuda_available = False\n",
        "print(cuda_available)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ8ruiVDJtMJ"
      },
      "source": [
        "dlossesTR    = {}\n",
        "dlossesTRAll = {}\n",
        "dlossesTE    = {}\n",
        "didxEpoch    = {}\n",
        "dbestAcc     = {}\n",
        "dnbParam     = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M88BlsKPJzy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cf67057-758d-4c07-d5be-c873c1207f91"
      },
      "source": [
        "net = NetVgg16()\n",
        "#net = models.vgg16(pretrained=False)\n",
        "#net = models.resnext50_32x4d(pretrained=False)\n",
        "init_params(net)\n",
        "lr0 = 1e-05               #1e-05\n",
        "lr_step  = 5\n",
        "lr_gamma = 1\n",
        "\n",
        "test_name = \"testvgg\"#net.name + '_lr0_' + \"{:.1e}\".format(lr0)\n",
        "if(cuda_available):\n",
        "  net = net.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialisation net VGG16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIQBjdEgKK8L"
      },
      "source": [
        "lossesTR    = []\n",
        "lossesTRAll = []\n",
        "lossesTE    = []\n",
        "idxEpoch    = [0]\n",
        "bestAcc     = 0\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr0)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXbQT_KPKTeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d8a0ec2-4226-49c7-a469-8e26045c8d69"
      },
      "source": [
        "\n",
        "for epoch in range(500):\n",
        "    #torch.cuda.empty_cache()\n",
        "    # potentially decrease lr \n",
        "    # scheduler.step()\n",
        "    lr = lr0 * lr_gamma**int(epoch/lr_step)\n",
        "    optimizer.lr = lr\n",
        "    print (lr)\n",
        "    \n",
        "    losses = []\n",
        "    # Train : 1 epoch <-> loop once one the entire training dataset\n",
        "    start = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if cuda_available:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        #print(batch_idx)\n",
        "        # clear gradient    \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # convert input to Variable\n",
        "        #inputs, targets = Variable(inputs), Variable(targets)\n",
        "        \n",
        "        # compute the output of the network for the given inputs\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        # compute the loss function\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # compute the gradient w.r. to all weights \n",
        "        loss.backward()\n",
        "        \n",
        "        # one update of the parameter update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # store loss of the current iterate\n",
        "        losses.append(loss.data.item())\n",
        "        lossesTRAll.append(loss.data.item())\n",
        "    \n",
        "    end = time.time()\n",
        "    # meanlosses = torch.mean(torch.stack(losses)) \n",
        "    lossesTR.append(np.mean(losses))\n",
        "    idxEpoch.append(idxEpoch[-1] + len(losses))\n",
        "    print('Epoch : %d Train Loss : %.3f         time: %.3f' % (epoch, np.mean(losses),end-start))\n",
        "    \n",
        "    # Evaluate the current network on the validation dataset\n",
        "    net.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    losses = []\n",
        "    start = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        if cuda_available:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets, volatile=True)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        losses.append(loss.data.item())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    end = time.time()\n",
        "    lossesTE.append(np.mean(losses))\n",
        "\n",
        "    bestAcc = max(bestAcc,100.*correct/total)\n",
        "    print('Epoch : %d Test Loss  : %.3f        Test Acc %.3f       time: %.3f' % (epoch, np.mean(losses),100.*correct/total,end-start))\n",
        "    print('--------------------------------------------------------------')\n",
        "    net.train()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1e-05\n",
            "Epoch : 0 Train Loss : 3.351         time: 35.985\n",
            "Epoch : 0 Test Loss  : 3.328        Test Acc 12.226       time: 1.810\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 1 Train Loss : 3.323         time: 35.900\n",
            "Epoch : 1 Test Loss  : 3.321        Test Acc 12.956       time: 1.815\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 2 Train Loss : 3.311         time: 35.856\n",
            "Epoch : 2 Test Loss  : 3.273        Test Acc 16.788       time: 1.795\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 3 Train Loss : 3.285         time: 35.928\n",
            "Epoch : 3 Test Loss  : 3.286        Test Acc 16.241       time: 1.803\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 4 Train Loss : 3.282         time: 35.876\n",
            "Epoch : 4 Test Loss  : 3.282        Test Acc 16.971       time: 1.777\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 5 Train Loss : 3.279         time: 35.953\n",
            "Epoch : 5 Test Loss  : 3.287        Test Acc 16.241       time: 1.796\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 6 Train Loss : 3.267         time: 35.945\n",
            "Epoch : 6 Test Loss  : 3.269        Test Acc 19.708       time: 1.790\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 7 Train Loss : 3.233         time: 35.938\n",
            "Epoch : 7 Test Loss  : 3.221        Test Acc 22.810       time: 1.786\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 8 Train Loss : 3.223         time: 35.954\n",
            "Epoch : 8 Test Loss  : 3.227        Test Acc 22.628       time: 1.806\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 9 Train Loss : 3.219         time: 35.881\n",
            "Epoch : 9 Test Loss  : 3.219        Test Acc 22.263       time: 1.820\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 10 Train Loss : 3.219         time: 35.960\n",
            "Epoch : 10 Test Loss  : 3.221        Test Acc 23.358       time: 1.802\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 11 Train Loss : 3.210         time: 35.954\n",
            "Epoch : 11 Test Loss  : 3.183        Test Acc 26.277       time: 1.797\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 12 Train Loss : 3.190         time: 35.927\n",
            "Epoch : 12 Test Loss  : 3.194        Test Acc 26.277       time: 1.811\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 13 Train Loss : 3.187         time: 35.928\n",
            "Epoch : 13 Test Loss  : 3.182        Test Acc 25.912       time: 1.798\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 14 Train Loss : 3.185         time: 35.882\n",
            "Epoch : 14 Test Loss  : 3.178        Test Acc 26.642       time: 1.798\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 15 Train Loss : 3.182         time: 35.879\n",
            "Epoch : 15 Test Loss  : 3.188        Test Acc 27.190       time: 1.821\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 16 Train Loss : 3.181         time: 35.985\n",
            "Epoch : 16 Test Loss  : 3.187        Test Acc 27.007       time: 1.817\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 17 Train Loss : 3.177         time: 35.938\n",
            "Epoch : 17 Test Loss  : 3.196        Test Acc 27.372       time: 1.796\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 18 Train Loss : 3.176         time: 35.978\n",
            "Epoch : 18 Test Loss  : 3.190        Test Acc 26.825       time: 1.798\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 19 Train Loss : 3.175         time: 35.936\n",
            "Epoch : 19 Test Loss  : 3.157        Test Acc 27.737       time: 1.792\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 20 Train Loss : 3.172         time: 35.906\n",
            "Epoch : 20 Test Loss  : 3.183        Test Acc 27.555       time: 1.793\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 21 Train Loss : 3.169         time: 35.991\n",
            "Epoch : 21 Test Loss  : 3.174        Test Acc 28.650       time: 1.817\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 22 Train Loss : 3.166         time: 35.911\n",
            "Epoch : 22 Test Loss  : 3.158        Test Acc 27.737       time: 1.787\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 23 Train Loss : 3.167         time: 35.965\n",
            "Epoch : 23 Test Loss  : 3.177        Test Acc 26.642       time: 1.813\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 24 Train Loss : 3.167         time: 35.922\n",
            "Epoch : 24 Test Loss  : 3.178        Test Acc 28.102       time: 1.806\n",
            "--------------------------------------------------------------\n",
            "1e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JV9lO3-KXkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "8195cced-8b91-4ec7-b9c4-5cc73200072f"
      },
      "source": [
        "\n",
        "lossesTR\n",
        "dlossesTR[test_name]    = lossesTR\n",
        "dlossesTRAll[test_name] = lossesTRAll\n",
        "dlossesTE[test_name]    = lossesTE\n",
        "didxEpoch[test_name]    = idxEpoch\n",
        "dbestAcc[test_name]     = bestAcc\n",
        "dnbParam[test_name]     = count_parameters(net)\n",
        "\n",
        "#for n in ['convnet', 'CNN2']:\n",
        "for n in [ test_name ]: # ajouter dans cette liste le nom des reseaux que vous tester \n",
        "    if n in dlossesTR:\n",
        "        print ('----------------------------------------------------------------------------')\n",
        "        print ('----------------------------------------------------------------------------')\n",
        "        print ('----------------------------------------------------------------------------')\n",
        "        print (n)\n",
        "        print ('best accuracy      : '+str(dbestAcc[n].item()))\n",
        "        print ('best loss on train : '+str(np.min(dlossesTR[n])) + ' idx '+str(np.argmin(dlossesTR[n])))\n",
        "        print ('best loss on test  : '+str(np.min(dlossesTE[n])) + ' idx '+str(np.argmin(dlossesTE[n])))\n",
        "        print ('n param            : '+str(dnbParam[n]))\n",
        "\n",
        "        # evenly sampled time at 200ms intervals\n",
        "        t = np.arange(0, len(dlossesTRAll[n]))\n",
        "\n",
        "        plt.plot(t, dlossesTRAll[n], 'b', didxEpoch[n][1:], dlossesTR[n], 'ro', didxEpoch[n][1:], dlossesTE[n], 'gs')\n",
        "        plt.title(n)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-638bd65e5766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlossesTR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdlossesTR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mlossesTR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdlossesTRAll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossesTRAll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdlossesTE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mlossesTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lossesTR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ74HmTGgQyQ"
      },
      "source": [
        " for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "   break\n",
        "print(batch_idx)\n",
        "output = net(inputs.cuda())\n",
        "print(output[0])\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2tkLasjHgh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}