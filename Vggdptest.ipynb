{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vggdptest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theolouvet/DatasetTest/blob/master/Vggdptest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJCHgod8EbA8"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Normalize, Compose\n",
        "import tarfile\n",
        "\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "import io\n",
        "import cv2 \n",
        "import os\n",
        "import sys\n",
        "from google.colab import files\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from io import BytesIO\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAgjgkI1EhDB"
      },
      "source": [
        "def count_parameters(model):\n",
        "    #for parameter in model.parameters():\n",
        "    #    print(parameter)\n",
        "    #print ('nb of trainable parameters')\n",
        "    return (sum([p.numel() for p in model.parameters() if p.requires_grad]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps73cXw7Puhx"
      },
      "source": [
        "class Subset(torch.utils.data.Dataset):\n",
        "      def __init__(self, dataset, indices):\n",
        "          self.dataset = dataset\n",
        "          self.indices = indices\n",
        "      def __getitem__(self, idx):\n",
        "          return self.dataset[self.indices[idx]]\n",
        "      def __len__(self):\n",
        "          return len(self.indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwplB6uNRh03"
      },
      "source": [
        "def padding(img,expected_size):\n",
        "    desired_size = expected_size\n",
        "    delta_width = desired_size - img.size[0]\n",
        "    delta_height = desired_size - img.size[1]\n",
        "    pad_width = delta_width //2\n",
        "    pad_height = delta_height //2\n",
        "    padding = (pad_width,pad_height,delta_width-pad_width,delta_height-pad_height)\n",
        "    return ImageOps.expand(img, padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2eqL5dyEqU6"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, txt_path='/content/Output_seuillage/small_training_dataset.csv'\n",
        ", img_dir='/content/Output_seuillage/', transform=None, test=False):\n",
        "     # initialize variables such is path to csv file and images and transforms\n",
        "    df = pd.read_csv(txt_path)\n",
        "   \n",
        "    #self.categories = categories\n",
        "    self.img_names = df.ID.values\n",
        "    self.format_img = '.jpg'\n",
        "    self.species = df.species.values\n",
        "    self.txt_path = txt_path\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.to_tensor = ToTensor()\n",
        "    self.to_pil = ToPILImage()\n",
        "\n",
        "    self.get_image_selector = True if img_dir.__contains__('tar') else False\n",
        "    self.tf = tarfile.open(self.img_dir) if self.get_image_selector else None\n",
        "    self.transform_gt = transform if test else Compose(self.transform.transforms[:-1])  # omit noise of ground truth\n",
        "    \n",
        "    print(\"dataset initialise\")\n",
        "  def set_categories(self, cat):\n",
        "    self.categories = cat\n",
        "  def __len__(self):\n",
        "      # here you just need to return a single integer number as the length of your dataset, in your \n",
        "      #  case, number of images in your train folder or lines in csv file\n",
        "    return len(self.img_names)\n",
        "      #return 1;\n",
        "  def get_image_from_folder(self, name):\n",
        "        \"\"\"\n",
        "        gets a image by a name gathered from file list text file\n",
        "        :param name: name of targeted image\n",
        "        :return: a PIL image\n",
        "        \"\"\"\n",
        "        image_name = os.path.join(self.img_dir, str(name))\n",
        "        image_name =image_name + str(self.format_img)\n",
        "        image = Image.open(image_name)\n",
        "        #image = plt.imread(image_name)\n",
        "        #print(image.size())\n",
        "        newsize = 256 \n",
        "        resize = (224, 224)\n",
        "        image = padding(image, newsize)\n",
        "        image = image.resize(resize) \n",
        "        trans1 = transforms.ToTensor()\n",
        "      \n",
        "        return trans1(image)\n",
        "  def getTarget(self, index):\n",
        "    return self.categories.get(self.species[index])\n",
        "  def getNameSpecies(self, idDictionary):\n",
        "    return list(test.keys())[list(test.values()).index(idDictionary)]\n",
        "  def show(self, index):\n",
        "    trans = transforms.ToPILImage()\n",
        "    img = self.get_image_from_folder(self.img_names[index])\n",
        "    plt.imshow(trans(img))\n",
        "  def __getitem__(self, index): \n",
        "    if index == (self.__len__() - 1) and self.get_image_selector:  # close tarfile opened in __init__\n",
        "              self.tf.close()\n",
        "    img = self.get_image_from_folder(self.img_names[index])\n",
        "    #spc = self.species[index]\n",
        "    spc = self.getTarget(index)\n",
        "    \n",
        "    sample = (img, spc)\n",
        "    \n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J51CDeAcoAcu"
      },
      "source": [
        "def zip_extraction_mf(file_name, filename_test, path_csv, path_csv_test):\n",
        "  with ZipFile(file_name_test, 'r') as zip:\n",
        "    zip.extractall();\n",
        "\n",
        "  with ZipFile(file_name, 'r') as zip:\n",
        "    zip.extractall();\n",
        "    print('done')\n",
        "  csv = pd.read_csv(path_csv)\n",
        "  csv.columns = [c.replace(' ', '_') for c in csv.columns]\n",
        "\n",
        "  csv_test = pd.read_csv(path_csv_test)\n",
        "  csv_test.columns = [c.replace(' ', '_') for c in csv.columns]\n",
        "\n",
        "  return csv, csv_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A2zwOM3RXL2"
      },
      "source": [
        "class NetVgg16(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NetVgg16,self).__init__()\n",
        "    print(\"initialisation net VGG16\")\n",
        "    self.name = 'vgg'\n",
        "    #block1\n",
        "    self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(1, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU())\n",
        "    \n",
        "    self.conv2 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(64, 128, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(128, 128, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv3 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(128, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv4 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(256, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    self.mp5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ))  #torch.Size([64, 512, 6, 6])\n",
        "    \n",
        "  \n",
        "    D_in, H, D_out = 512*7*7, 1000, 30\n",
        "\n",
        "    self.fconnected = nn.Sequential(\n",
        "                      nn.Flatten(),\n",
        "                      torch.nn.Linear(D_in, H),\n",
        "                      torch.nn.ReLU(),\n",
        "                      torch.nn.Linear(H, D_out))\n",
        "                      #nn.Linear(512 * 7 * 7 , 100),\n",
        "                      #nn.ReLU())\n",
        "                      #torch.nn.Linear(1000, 50))\n",
        "\n",
        "    self.sm = nn.Softmax(dim = 1)\n",
        "    # Block 2\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    \n",
        "    x = self.conv3(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    bs = self.conv5(x).size()[0]\n",
        "    \n",
        "    x = self.conv5(x)\n",
        "\n",
        "    x = self.mp5(x)\n",
        "\n",
        "    x  = x.view(bs,-1)\n",
        "\n",
        "    x = self.fconnected(x)\n",
        "    \n",
        "    x = self.sm(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qSIFmuLRDNa"
      },
      "source": [
        "class NetVgg19(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NetVgg19,self).__init__()\n",
        "    print(\"initialisation net VGG19\")\n",
        "    self.name = 'vgg'\n",
        "    #block1\n",
        "    self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(1, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU())\n",
        "    \n",
        "    self.conv2 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(64, 128, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(128, 128, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv3 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(128, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(256, 256, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv4 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(256, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    \n",
        "    self.conv5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(512, 512, 3, padding=1),\n",
        "                      nn.ReLU())\n",
        "    self.mp5 = nn.Sequential(\n",
        "                      nn.MaxPool2d((2,2) , (2,2) ))  #torch.Size([64, 512, 6, 6])\n",
        "    \n",
        "  \n",
        "    D_in, H, D_out = 25088, 1000, 50\n",
        "\n",
        "    self.fconnected = nn.Sequential(\n",
        "                      nn.Flatten(),\n",
        "                      torch.nn.Linear(D_in, H),\n",
        "                      torch.nn.ReLU(),\n",
        "                      torch.nn.Linear(H, D_out))\n",
        "                      #nn.Linear(512 * 7 * 7 , 100),\n",
        "                      #nn.ReLU())\n",
        "                      #torch.nn.Linear(1000, 50))\n",
        "\n",
        "    self.sm = nn.Softmax(dim = 1)\n",
        "    # Block 2\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    \n",
        "    x = self.conv3(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    bs = self.conv5(x).size()[0]\n",
        "    \n",
        "    x = self.conv5(x)\n",
        "\n",
        "    x = self.mp5(x)\n",
        "\n",
        "    x  = x.view(bs,-1)\n",
        "\n",
        "    x = self.fconnected(x)\n",
        "    \n",
        "    x = self.sm(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1B4JLVgE7f3"
      },
      "source": [
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        #print(m)\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            #if m.bias:\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant_(m.weight, 1)\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal_(m.weight, std=1e-3)\n",
        "            #if m.bias:\n",
        "            init.constant_(m.bias, 0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDSxF8fQFA1y"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "file_name = \"training_dataset_50_90_pourcent.zip\"\n",
        "file_name_test = \"testing_dataset_50_90_pourcent.zip\"\n",
        "file_name = \"train_50_DL.zip\"\n",
        "file_name_test = \"test_50_DL.zip\"\n",
        "path_csv_test = '/content/test/test_dataset_50_90_pourcent.csv'\n",
        "path_csv_test = '/content/test/testing_Dataset_50_seuillage_200.csv'\n",
        "path_csv = '/content/train/train_dataset_50_90_pourcent.csv'\n",
        "path_csv= '/content/train/training_Dataset_50_seuillage_200.csv'\n",
        "\n",
        "\n",
        "file_name = \"/content/train_30_90_10_VeinSegm_256.zip\"\n",
        "file_name_test = \"/content/test_30_90_10_VeinSgem_256.zip\"\n",
        "\n",
        "path_csv_test = '/content/test/test_dataset_30_90_10.csv'\n",
        "path_csv = '/content/train/train_dataset_30_90_10.csv'\n",
        "\n",
        "\n",
        "file_name = \"/content/train_30_90_10_seuillage_256.zip\"\n",
        "file_name_test = \"/content/test_30_90_10_seuillage_256.zip\"\n",
        "\n",
        "path_csv_test = '/content/test/test_dataset_30_90_10.csv'\n",
        "path_csv = '/content/train/train_dataset_30_90_10.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Qt77jXFCkQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50914f5b-d1ff-46e1-da1c-9e6da5fdd5d8"
      },
      "source": [
        "csv, csv_test = zip_extraction_mf(file_name, file_name_test, path_csv, path_csv_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJSbr-IlEwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "be33d307-9f08-4d65-c3f2-38946b94adef"
      },
      "source": [
        "df = pd.read_csv(path_csv)\n",
        "categories = {}\n",
        "i = 0\n",
        "for row in df.species.values:\n",
        "  if row not in categories:\n",
        "    categories[row] = i\n",
        "    i = i + 1\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Populus nigra': 0, 'Viburnum tinus': 1, 'Rhus coriaria': 2, 'Phillyrea angustifolia': 3, 'Pittosporum tenuifolium': 4, 'Quercus ilex': 5, 'Pittosporum tobira': 6, 'Populus alba': 7, 'Betula pendula': 8, 'Hedera helix': 9, 'Crataegus monogyna': 10, 'Olea europaea': 11, 'Arbutus unedo': 12, 'Acer monspessulanum': 13, 'Daphne cneorum': 14, 'Juniperus oxycedrus': 15, 'Pistacia lentiscus': 16, 'Buxus sempervirens': 17, 'Buddleja davidii': 18, 'Ginkgo biloba': 19, 'Ulmus minor': 20, 'Celtis australis': 21, 'Euphorbia characias': 22, 'Carpinus betulus': 23, 'Acer campestre': 24, 'Ruscus aculeatus': 25, 'Punica granatum': 26, 'Cercis siliquastrum': 27, 'Cotinus coggygria': 28, 'Rhamnus alaternus': 29}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvYotOmQFMmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "953c3d96-9c16-40a9-8308-dee92699074e"
      },
      "source": [
        "custom_transforms = Compose([ ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset= ImageDataset(txt_path=path_csv,\n",
        "                              img_dir='/content/train/',\n",
        "                              transform= custom_transforms,\n",
        "\n",
        "                              test=False)\n",
        "train_dataset.set_categories(categories)\n",
        "\n",
        "test_dataset= ImageDataset(txt_path=path_csv_test,\n",
        "                              img_dir='/content/test/',\n",
        "                              transform= custom_transforms,\n",
        "\n",
        "                              test=False)\n",
        "test_dataset.set_categories(categories)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset initialise\n",
            "dataset initialise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgyHgX1cFQut"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "#structure de pytorch facilitant l acces a une base de donn√©e\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik_9elQGFeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "dfbe7196-3746-4474-ba07-19ceac955190"
      },
      "source": [
        "'''\n",
        "net = NetVgg16()\n",
        "net = net.cuda()\n",
        "for batch_idx, (inputs,targets) in enumerate(trainloader):\n",
        "  break\n",
        "inputs = inputs.cuda()\n",
        "print(inputs.size())\n",
        "output = net(inputs)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnet = NetVgg16()\\nnet = net.cuda()\\nfor batch_idx, (inputs,targets) in enumerate(trainloader):\\n  break\\ninputs = inputs.cuda()\\nprint(inputs.size())\\noutput = net(inputs)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPaYTS_nVFhW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "90e902d6-e609-4346-f116-2a2621852a9a"
      },
      "source": [
        "'''image = Image.open('/content/test/10054.jpg')\n",
        "#image = plt.imread(image_name)\n",
        "#print(image.size())\n",
        "newsize = 256\n",
        "padding(image, newsize)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"image = Image.open('/content/test/10054.jpg')\\n#image = plt.imread(image_name)\\n#print(image.size())\\nnewsize = 256\\npadding(image, newsize)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9dZUnt9JgAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ec8466e-1401-466a-b2dc-6208c7b0a06e"
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "#cuda_available = False\n",
        "print(cuda_available)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ8ruiVDJtMJ"
      },
      "source": [
        "dlossesTR    = {}\n",
        "dlossesTRAll = {}\n",
        "dlossesTE    = {}\n",
        "didxEpoch    = {}\n",
        "dbestAcc     = {}\n",
        "dnbParam     = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M88BlsKPJzy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4148c57d-3ff5-4859-d664-c816a25616ae"
      },
      "source": [
        "net = NetVgg16()\n",
        "init_params(net)\n",
        "lr0 = 1e-05               #1e-05\n",
        "lr_step  = 50\n",
        "lr_gamma = 0.1\n",
        "test_name = net.name + '_lr0_' + \"{:.1e}\".format(lr0)\n",
        "if(cuda_available):\n",
        "  net = net.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialisation net VGG16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIQBjdEgKK8L"
      },
      "source": [
        "lossesTR    = []\n",
        "lossesTRAll = []\n",
        "lossesTE    = []\n",
        "idxEpoch    = [0]\n",
        "bestAcc     = 0\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr0)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXbQT_KPKTeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88fca4f0-c8ab-4709-f971-7700f846908f"
      },
      "source": [
        "\n",
        "for epoch in range(200):\n",
        "    #torch.cuda.empty_cache()\n",
        "    # potentially decrease lr \n",
        "    # scheduler.step()\n",
        "    lr = lr0 * lr_gamma**int(epoch/lr_step)\n",
        "    optimizer.lr = lr\n",
        "    print (lr)\n",
        "    \n",
        "    losses = []\n",
        "    # Train : 1 epoch <-> loop once one the entire training dataset\n",
        "    start = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if cuda_available:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        #print(batch_idx)\n",
        "        # clear gradient    \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # convert input to Variable\n",
        "        #inputs, targets = Variable(inputs), Variable(targets)\n",
        "        \n",
        "        # compute the output of the network for the given inputs\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        # compute the loss function\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # compute the gradient w.r. to all weights \n",
        "        loss.backward()\n",
        "        \n",
        "        # one update of the parameter update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # store loss of the current iterate\n",
        "        losses.append(loss.data.item())\n",
        "        lossesTRAll.append(loss.data.item())\n",
        "    \n",
        "    end = time.time()\n",
        "    # meanlosses = torch.mean(torch.stack(losses)) \n",
        "    lossesTR.append(np.mean(losses))\n",
        "    idxEpoch.append(idxEpoch[-1] + len(losses))\n",
        "    print('Epoch : %d Train Loss : %.3f         time: %.3f' % (epoch, np.mean(losses),end-start))\n",
        "    \n",
        "    # Evaluate the current network on the validation dataset\n",
        "    net.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    losses = []\n",
        "    start = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        if cuda_available:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets, volatile=True)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        losses.append(loss.data.item())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "    end = time.time()\n",
        "    lossesTE.append(np.mean(losses))\n",
        "\n",
        "    bestAcc = max(bestAcc,100.*correct/total)\n",
        "    print('Epoch : %d Test Loss  : %.3f        Test Acc %.3f       time: %.3f' % (epoch, np.mean(losses),100.*correct/total,end-start))\n",
        "    print('--------------------------------------------------------------')\n",
        "    net.train()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1e-05\n",
            "Epoch : 0 Train Loss : 3.332         time: 35.198\n",
            "Epoch : 0 Test Loss  : 3.248        Test Acc 19.343       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 1 Train Loss : 3.261         time: 35.145\n",
            "Epoch : 1 Test Loss  : 3.275        Test Acc 18.978       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 2 Train Loss : 3.255         time: 35.121\n",
            "Epoch : 2 Test Loss  : 3.287        Test Acc 17.701       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 3 Train Loss : 3.247         time: 35.119\n",
            "Epoch : 3 Test Loss  : 3.260        Test Acc 20.803       time: 1.653\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 4 Train Loss : 3.238         time: 35.163\n",
            "Epoch : 4 Test Loss  : 3.215        Test Acc 21.898       time: 1.654\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 5 Train Loss : 3.234         time: 35.133\n",
            "Epoch : 5 Test Loss  : 3.237        Test Acc 21.715       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 6 Train Loss : 3.220         time: 35.147\n",
            "Epoch : 6 Test Loss  : 3.201        Test Acc 24.453       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 7 Train Loss : 3.208         time: 35.126\n",
            "Epoch : 7 Test Loss  : 3.243        Test Acc 22.263       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 8 Train Loss : 3.203         time: 35.174\n",
            "Epoch : 8 Test Loss  : 3.217        Test Acc 24.270       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 9 Train Loss : 3.200         time: 35.171\n",
            "Epoch : 9 Test Loss  : 3.207        Test Acc 23.723       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 10 Train Loss : 3.192         time: 35.164\n",
            "Epoch : 10 Test Loss  : 3.217        Test Acc 25.182       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 11 Train Loss : 3.188         time: 35.137\n",
            "Epoch : 11 Test Loss  : 3.182        Test Acc 25.182       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 12 Train Loss : 3.191         time: 35.180\n",
            "Epoch : 12 Test Loss  : 3.197        Test Acc 26.277       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 13 Train Loss : 3.185         time: 35.168\n",
            "Epoch : 13 Test Loss  : 3.208        Test Acc 25.912       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 14 Train Loss : 3.186         time: 35.164\n",
            "Epoch : 14 Test Loss  : 3.192        Test Acc 26.825       time: 1.651\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 15 Train Loss : 3.184         time: 35.172\n",
            "Epoch : 15 Test Loss  : 3.195        Test Acc 26.095       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 16 Train Loss : 3.186         time: 35.171\n",
            "Epoch : 16 Test Loss  : 3.181        Test Acc 26.460       time: 1.654\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 17 Train Loss : 3.180         time: 35.214\n",
            "Epoch : 17 Test Loss  : 3.205        Test Acc 26.277       time: 1.651\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 18 Train Loss : 3.180         time: 35.177\n",
            "Epoch : 18 Test Loss  : 3.204        Test Acc 26.642       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 19 Train Loss : 3.183         time: 35.187\n",
            "Epoch : 19 Test Loss  : 3.197        Test Acc 25.912       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 20 Train Loss : 3.177         time: 35.182\n",
            "Epoch : 20 Test Loss  : 3.205        Test Acc 26.277       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 21 Train Loss : 3.178         time: 35.202\n",
            "Epoch : 21 Test Loss  : 3.166        Test Acc 26.642       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 22 Train Loss : 3.180         time: 35.185\n",
            "Epoch : 22 Test Loss  : 3.203        Test Acc 26.825       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 23 Train Loss : 3.178         time: 35.188\n",
            "Epoch : 23 Test Loss  : 3.194        Test Acc 26.277       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 24 Train Loss : 3.176         time: 35.165\n",
            "Epoch : 24 Test Loss  : 3.201        Test Acc 26.825       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 25 Train Loss : 3.175         time: 35.211\n",
            "Epoch : 25 Test Loss  : 3.179        Test Acc 26.642       time: 1.652\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 26 Train Loss : 3.177         time: 35.178\n",
            "Epoch : 26 Test Loss  : 3.203        Test Acc 26.642       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 27 Train Loss : 3.174         time: 35.193\n",
            "Epoch : 27 Test Loss  : 3.182        Test Acc 26.825       time: 1.663\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 28 Train Loss : 3.173         time: 35.201\n",
            "Epoch : 28 Test Loss  : 3.201        Test Acc 26.825       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 29 Train Loss : 3.174         time: 35.200\n",
            "Epoch : 29 Test Loss  : 3.194        Test Acc 26.095       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 30 Train Loss : 3.174         time: 35.171\n",
            "Epoch : 30 Test Loss  : 3.192        Test Acc 26.277       time: 1.669\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 31 Train Loss : 3.172         time: 35.206\n",
            "Epoch : 31 Test Loss  : 3.199        Test Acc 26.825       time: 1.653\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 32 Train Loss : 3.171         time: 35.203\n",
            "Epoch : 32 Test Loss  : 3.199        Test Acc 27.007       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 33 Train Loss : 3.169         time: 35.223\n",
            "Epoch : 33 Test Loss  : 3.198        Test Acc 27.190       time: 1.671\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 34 Train Loss : 3.169         time: 35.200\n",
            "Epoch : 34 Test Loss  : 3.197        Test Acc 27.007       time: 1.654\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 35 Train Loss : 3.152         time: 35.199\n",
            "Epoch : 35 Test Loss  : 3.142        Test Acc 30.292       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 36 Train Loss : 3.133         time: 35.223\n",
            "Epoch : 36 Test Loss  : 3.149        Test Acc 31.204       time: 1.682\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 37 Train Loss : 3.134         time: 35.237\n",
            "Epoch : 37 Test Loss  : 3.141        Test Acc 30.474       time: 1.651\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 38 Train Loss : 3.133         time: 35.224\n",
            "Epoch : 38 Test Loss  : 3.154        Test Acc 30.292       time: 1.654\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 39 Train Loss : 3.120         time: 35.192\n",
            "Epoch : 39 Test Loss  : 3.118        Test Acc 33.212       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 40 Train Loss : 3.102         time: 35.246\n",
            "Epoch : 40 Test Loss  : 3.126        Test Acc 33.394       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 41 Train Loss : 3.101         time: 35.234\n",
            "Epoch : 41 Test Loss  : 3.120        Test Acc 34.124       time: 1.672\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 42 Train Loss : 3.096         time: 35.202\n",
            "Epoch : 42 Test Loss  : 3.084        Test Acc 33.942       time: 1.653\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 43 Train Loss : 3.099         time: 35.232\n",
            "Epoch : 43 Test Loss  : 3.119        Test Acc 34.489       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 44 Train Loss : 3.098         time: 35.237\n",
            "Epoch : 44 Test Loss  : 3.106        Test Acc 34.672       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 45 Train Loss : 3.096         time: 35.228\n",
            "Epoch : 45 Test Loss  : 3.102        Test Acc 34.307       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 46 Train Loss : 3.093         time: 35.238\n",
            "Epoch : 46 Test Loss  : 3.102        Test Acc 34.672       time: 1.670\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 47 Train Loss : 3.092         time: 35.245\n",
            "Epoch : 47 Test Loss  : 3.103        Test Acc 34.489       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 48 Train Loss : 3.091         time: 35.239\n",
            "Epoch : 48 Test Loss  : 3.103        Test Acc 34.489       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1e-05\n",
            "Epoch : 49 Train Loss : 3.091         time: 35.234\n",
            "Epoch : 49 Test Loss  : 3.099        Test Acc 35.036       time: 1.653\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 50 Train Loss : 3.090         time: 35.218\n",
            "Epoch : 50 Test Loss  : 3.095        Test Acc 34.124       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 51 Train Loss : 3.089         time: 35.231\n",
            "Epoch : 51 Test Loss  : 3.092        Test Acc 34.307       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 52 Train Loss : 3.088         time: 35.216\n",
            "Epoch : 52 Test Loss  : 3.126        Test Acc 34.307       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 53 Train Loss : 3.086         time: 35.218\n",
            "Epoch : 53 Test Loss  : 3.112        Test Acc 34.672       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 54 Train Loss : 3.086         time: 35.238\n",
            "Epoch : 54 Test Loss  : 3.115        Test Acc 34.307       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 55 Train Loss : 3.086         time: 35.204\n",
            "Epoch : 55 Test Loss  : 3.116        Test Acc 34.307       time: 1.650\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 56 Train Loss : 3.087         time: 35.249\n",
            "Epoch : 56 Test Loss  : 3.099        Test Acc 34.672       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 57 Train Loss : 3.082         time: 35.250\n",
            "Epoch : 57 Test Loss  : 3.111        Test Acc 34.489       time: 1.654\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 58 Train Loss : 3.082         time: 35.237\n",
            "Epoch : 58 Test Loss  : 3.121        Test Acc 34.854       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 59 Train Loss : 3.082         time: 35.234\n",
            "Epoch : 59 Test Loss  : 3.108        Test Acc 35.036       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 60 Train Loss : 3.083         time: 35.224\n",
            "Epoch : 60 Test Loss  : 3.112        Test Acc 34.672       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 61 Train Loss : 3.081         time: 35.236\n",
            "Epoch : 61 Test Loss  : 3.109        Test Acc 35.036       time: 1.668\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 62 Train Loss : 3.082         time: 35.219\n",
            "Epoch : 62 Test Loss  : 3.122        Test Acc 34.854       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 63 Train Loss : 3.083         time: 35.214\n",
            "Epoch : 63 Test Loss  : 3.122        Test Acc 34.854       time: 1.646\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 64 Train Loss : 3.081         time: 35.239\n",
            "Epoch : 64 Test Loss  : 3.107        Test Acc 35.219       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 65 Train Loss : 3.081         time: 35.225\n",
            "Epoch : 65 Test Loss  : 3.090        Test Acc 34.124       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 66 Train Loss : 3.080         time: 35.209\n",
            "Epoch : 66 Test Loss  : 3.098        Test Acc 34.854       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 67 Train Loss : 3.078         time: 35.201\n",
            "Epoch : 67 Test Loss  : 3.117        Test Acc 35.219       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 68 Train Loss : 3.079         time: 35.216\n",
            "Epoch : 68 Test Loss  : 3.083        Test Acc 35.219       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 69 Train Loss : 3.079         time: 35.212\n",
            "Epoch : 69 Test Loss  : 3.097        Test Acc 35.036       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 70 Train Loss : 3.077         time: 35.225\n",
            "Epoch : 70 Test Loss  : 3.108        Test Acc 35.036       time: 1.670\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 71 Train Loss : 3.078         time: 35.217\n",
            "Epoch : 71 Test Loss  : 3.107        Test Acc 35.036       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 72 Train Loss : 3.077         time: 35.255\n",
            "Epoch : 72 Test Loss  : 3.095        Test Acc 35.036       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 73 Train Loss : 3.077         time: 35.240\n",
            "Epoch : 73 Test Loss  : 3.095        Test Acc 35.219       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 74 Train Loss : 3.077         time: 35.253\n",
            "Epoch : 74 Test Loss  : 3.093        Test Acc 35.219       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 75 Train Loss : 3.075         time: 35.207\n",
            "Epoch : 75 Test Loss  : 3.086        Test Acc 34.672       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 76 Train Loss : 3.076         time: 35.234\n",
            "Epoch : 76 Test Loss  : 3.087        Test Acc 34.307       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 77 Train Loss : 3.077         time: 35.230\n",
            "Epoch : 77 Test Loss  : 3.114        Test Acc 34.489       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 78 Train Loss : 3.077         time: 35.219\n",
            "Epoch : 78 Test Loss  : 3.096        Test Acc 35.036       time: 1.663\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 79 Train Loss : 3.075         time: 35.219\n",
            "Epoch : 79 Test Loss  : 3.096        Test Acc 34.854       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 80 Train Loss : 3.075         time: 35.255\n",
            "Epoch : 80 Test Loss  : 3.109        Test Acc 34.672       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 81 Train Loss : 3.074         time: 35.225\n",
            "Epoch : 81 Test Loss  : 3.073        Test Acc 34.854       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 82 Train Loss : 3.074         time: 35.247\n",
            "Epoch : 82 Test Loss  : 3.123        Test Acc 34.124       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 83 Train Loss : 3.073         time: 35.239\n",
            "Epoch : 83 Test Loss  : 3.120        Test Acc 34.307       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 84 Train Loss : 3.073         time: 35.255\n",
            "Epoch : 84 Test Loss  : 3.097        Test Acc 34.672       time: 1.670\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 85 Train Loss : 3.074         time: 35.231\n",
            "Epoch : 85 Test Loss  : 3.120        Test Acc 34.854       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 86 Train Loss : 3.072         time: 35.246\n",
            "Epoch : 86 Test Loss  : 3.098        Test Acc 34.489       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 87 Train Loss : 3.073         time: 35.220\n",
            "Epoch : 87 Test Loss  : 3.101        Test Acc 34.489       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 88 Train Loss : 3.073         time: 35.243\n",
            "Epoch : 88 Test Loss  : 3.086        Test Acc 34.489       time: 1.675\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 89 Train Loss : 3.073         time: 35.244\n",
            "Epoch : 89 Test Loss  : 3.109        Test Acc 34.489       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 90 Train Loss : 3.072         time: 35.237\n",
            "Epoch : 90 Test Loss  : 3.106        Test Acc 35.036       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 91 Train Loss : 3.068         time: 35.226\n",
            "Epoch : 91 Test Loss  : 3.095        Test Acc 35.766       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 92 Train Loss : 3.041         time: 35.228\n",
            "Epoch : 92 Test Loss  : 3.068        Test Acc 39.416       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 93 Train Loss : 3.028         time: 35.246\n",
            "Epoch : 93 Test Loss  : 3.042        Test Acc 39.234       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 94 Train Loss : 3.017         time: 35.260\n",
            "Epoch : 94 Test Loss  : 3.063        Test Acc 39.781       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 95 Train Loss : 3.008         time: 35.228\n",
            "Epoch : 95 Test Loss  : 3.061        Test Acc 39.781       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 96 Train Loss : 3.005         time: 35.231\n",
            "Epoch : 96 Test Loss  : 3.039        Test Acc 41.058       time: 1.649\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 97 Train Loss : 3.004         time: 35.233\n",
            "Epoch : 97 Test Loss  : 3.027        Test Acc 41.058       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 98 Train Loss : 3.001         time: 35.207\n",
            "Epoch : 98 Test Loss  : 3.053        Test Acc 40.876       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-06\n",
            "Epoch : 99 Train Loss : 2.998         time: 35.241\n",
            "Epoch : 99 Test Loss  : 3.054        Test Acc 40.693       time: 1.667\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 100 Train Loss : 2.999         time: 35.245\n",
            "Epoch : 100 Test Loss  : 3.038        Test Acc 41.058       time: 1.649\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 101 Train Loss : 2.996         time: 35.245\n",
            "Epoch : 101 Test Loss  : 3.029        Test Acc 40.511       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 102 Train Loss : 2.994         time: 35.225\n",
            "Epoch : 102 Test Loss  : 3.052        Test Acc 40.511       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 103 Train Loss : 2.995         time: 35.266\n",
            "Epoch : 103 Test Loss  : 3.057        Test Acc 39.964       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 104 Train Loss : 2.996         time: 35.249\n",
            "Epoch : 104 Test Loss  : 3.029        Test Acc 41.058       time: 1.653\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 105 Train Loss : 2.994         time: 35.245\n",
            "Epoch : 105 Test Loss  : 3.049        Test Acc 39.599       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 106 Train Loss : 2.997         time: 35.239\n",
            "Epoch : 106 Test Loss  : 3.050        Test Acc 41.058       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 107 Train Loss : 2.993         time: 35.239\n",
            "Epoch : 107 Test Loss  : 3.052        Test Acc 40.876       time: 1.663\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 108 Train Loss : 2.992         time: 35.219\n",
            "Epoch : 108 Test Loss  : 3.040        Test Acc 40.876       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 109 Train Loss : 2.991         time: 35.260\n",
            "Epoch : 109 Test Loss  : 3.051        Test Acc 40.876       time: 1.651\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 110 Train Loss : 2.991         time: 35.212\n",
            "Epoch : 110 Test Loss  : 3.041        Test Acc 40.146       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 111 Train Loss : 2.991         time: 35.249\n",
            "Epoch : 111 Test Loss  : 3.041        Test Acc 40.693       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 112 Train Loss : 2.988         time: 35.229\n",
            "Epoch : 112 Test Loss  : 3.038        Test Acc 41.971       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 113 Train Loss : 2.982         time: 35.245\n",
            "Epoch : 113 Test Loss  : 3.022        Test Acc 42.701       time: 1.673\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 114 Train Loss : 2.979         time: 35.274\n",
            "Epoch : 114 Test Loss  : 3.008        Test Acc 42.883       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 115 Train Loss : 2.977         time: 35.264\n",
            "Epoch : 115 Test Loss  : 3.033        Test Acc 42.883       time: 1.652\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 116 Train Loss : 2.973         time: 35.226\n",
            "Epoch : 116 Test Loss  : 3.040        Test Acc 43.431       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 117 Train Loss : 2.971         time: 35.247\n",
            "Epoch : 117 Test Loss  : 3.032        Test Acc 42.701       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 118 Train Loss : 2.970         time: 35.246\n",
            "Epoch : 118 Test Loss  : 3.036        Test Acc 42.153       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 119 Train Loss : 2.971         time: 35.215\n",
            "Epoch : 119 Test Loss  : 3.035        Test Acc 41.971       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 120 Train Loss : 2.969         time: 35.248\n",
            "Epoch : 120 Test Loss  : 3.027        Test Acc 43.066       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 121 Train Loss : 2.968         time: 35.242\n",
            "Epoch : 121 Test Loss  : 3.028        Test Acc 43.066       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 122 Train Loss : 2.970         time: 35.243\n",
            "Epoch : 122 Test Loss  : 3.016        Test Acc 43.248       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 123 Train Loss : 2.969         time: 35.219\n",
            "Epoch : 123 Test Loss  : 3.017        Test Acc 42.701       time: 1.653\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 124 Train Loss : 2.970         time: 35.235\n",
            "Epoch : 124 Test Loss  : 3.029        Test Acc 43.248       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 125 Train Loss : 2.970         time: 35.240\n",
            "Epoch : 125 Test Loss  : 3.039        Test Acc 43.431       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 126 Train Loss : 2.969         time: 35.244\n",
            "Epoch : 126 Test Loss  : 3.033        Test Acc 42.518       time: 1.652\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 127 Train Loss : 2.970         time: 35.244\n",
            "Epoch : 127 Test Loss  : 3.028        Test Acc 43.248       time: 1.651\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 128 Train Loss : 2.969         time: 35.224\n",
            "Epoch : 128 Test Loss  : 3.016        Test Acc 43.613       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 129 Train Loss : 2.969         time: 35.232\n",
            "Epoch : 129 Test Loss  : 3.030        Test Acc 43.248       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 130 Train Loss : 2.970         time: 35.237\n",
            "Epoch : 130 Test Loss  : 3.036        Test Acc 43.796       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 131 Train Loss : 2.968         time: 35.243\n",
            "Epoch : 131 Test Loss  : 3.011        Test Acc 43.613       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 132 Train Loss : 2.967         time: 35.246\n",
            "Epoch : 132 Test Loss  : 3.001        Test Acc 43.978       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 133 Train Loss : 2.967         time: 35.230\n",
            "Epoch : 133 Test Loss  : 3.001        Test Acc 43.978       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 134 Train Loss : 2.967         time: 35.239\n",
            "Epoch : 134 Test Loss  : 3.025        Test Acc 43.796       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 135 Train Loss : 2.966         time: 35.223\n",
            "Epoch : 135 Test Loss  : 3.025        Test Acc 43.796       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 136 Train Loss : 2.966         time: 35.250\n",
            "Epoch : 136 Test Loss  : 3.001        Test Acc 43.978       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 137 Train Loss : 2.966         time: 35.238\n",
            "Epoch : 137 Test Loss  : 3.027        Test Acc 43.796       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 138 Train Loss : 2.967         time: 35.235\n",
            "Epoch : 138 Test Loss  : 3.027        Test Acc 43.613       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 139 Train Loss : 2.967         time: 35.262\n",
            "Epoch : 139 Test Loss  : 3.006        Test Acc 43.248       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 140 Train Loss : 2.967         time: 35.239\n",
            "Epoch : 140 Test Loss  : 3.025        Test Acc 43.796       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 141 Train Loss : 2.966         time: 35.244\n",
            "Epoch : 141 Test Loss  : 3.024        Test Acc 43.796       time: 1.663\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 142 Train Loss : 2.966         time: 35.264\n",
            "Epoch : 142 Test Loss  : 3.014        Test Acc 43.796       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 143 Train Loss : 2.974         time: 35.239\n",
            "Epoch : 143 Test Loss  : 3.039        Test Acc 41.058       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 144 Train Loss : 2.975         time: 35.260\n",
            "Epoch : 144 Test Loss  : 3.018        Test Acc 43.248       time: 1.671\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 145 Train Loss : 2.965         time: 35.240\n",
            "Epoch : 145 Test Loss  : 3.010        Test Acc 44.343       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 146 Train Loss : 2.961         time: 35.220\n",
            "Epoch : 146 Test Loss  : 2.997        Test Acc 44.161       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 147 Train Loss : 2.960         time: 35.232\n",
            "Epoch : 147 Test Loss  : 2.990        Test Acc 44.708       time: 1.671\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 148 Train Loss : 2.960         time: 35.238\n",
            "Epoch : 148 Test Loss  : 2.991        Test Acc 44.708       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000002e-07\n",
            "Epoch : 149 Train Loss : 2.959         time: 35.260\n",
            "Epoch : 149 Test Loss  : 3.015        Test Acc 44.343       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 150 Train Loss : 2.959         time: 35.242\n",
            "Epoch : 150 Test Loss  : 2.994        Test Acc 44.526       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 151 Train Loss : 2.959         time: 35.226\n",
            "Epoch : 151 Test Loss  : 3.018        Test Acc 44.343       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 152 Train Loss : 2.959         time: 35.235\n",
            "Epoch : 152 Test Loss  : 3.006        Test Acc 44.526       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 153 Train Loss : 2.959         time: 35.241\n",
            "Epoch : 153 Test Loss  : 3.006        Test Acc 44.526       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 154 Train Loss : 2.958         time: 35.254\n",
            "Epoch : 154 Test Loss  : 3.006        Test Acc 44.526       time: 1.672\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 155 Train Loss : 2.959         time: 35.250\n",
            "Epoch : 155 Test Loss  : 3.006        Test Acc 44.526       time: 1.663\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 156 Train Loss : 2.958         time: 35.255\n",
            "Epoch : 156 Test Loss  : 2.995        Test Acc 44.343       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 157 Train Loss : 2.959         time: 35.243\n",
            "Epoch : 157 Test Loss  : 2.995        Test Acc 44.343       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 158 Train Loss : 2.956         time: 35.276\n",
            "Epoch : 158 Test Loss  : 3.001        Test Acc 46.168       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 159 Train Loss : 2.942         time: 35.224\n",
            "Epoch : 159 Test Loss  : 2.995        Test Acc 45.620       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 160 Train Loss : 2.951         time: 35.228\n",
            "Epoch : 160 Test Loss  : 2.989        Test Acc 46.350       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 161 Train Loss : 2.942         time: 35.240\n",
            "Epoch : 161 Test Loss  : 3.010        Test Acc 46.168       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 162 Train Loss : 2.937         time: 35.205\n",
            "Epoch : 162 Test Loss  : 2.985        Test Acc 46.715       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 163 Train Loss : 2.934         time: 35.232\n",
            "Epoch : 163 Test Loss  : 2.980        Test Acc 47.080       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 164 Train Loss : 2.932         time: 35.216\n",
            "Epoch : 164 Test Loss  : 2.979        Test Acc 47.263       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 165 Train Loss : 2.931         time: 35.247\n",
            "Epoch : 165 Test Loss  : 2.982        Test Acc 47.080       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 166 Train Loss : 2.931         time: 35.235\n",
            "Epoch : 166 Test Loss  : 2.982        Test Acc 46.898       time: 1.649\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 167 Train Loss : 2.931         time: 35.230\n",
            "Epoch : 167 Test Loss  : 2.995        Test Acc 46.898       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 168 Train Loss : 2.930         time: 35.235\n",
            "Epoch : 168 Test Loss  : 2.982        Test Acc 46.898       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 169 Train Loss : 2.931         time: 35.241\n",
            "Epoch : 169 Test Loss  : 2.984        Test Acc 46.715       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 170 Train Loss : 2.931         time: 35.243\n",
            "Epoch : 170 Test Loss  : 2.959        Test Acc 46.898       time: 1.658\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 171 Train Loss : 2.930         time: 35.240\n",
            "Epoch : 171 Test Loss  : 2.981        Test Acc 47.080       time: 1.670\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 172 Train Loss : 2.930         time: 35.247\n",
            "Epoch : 172 Test Loss  : 2.994        Test Acc 46.715       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 173 Train Loss : 2.930         time: 35.270\n",
            "Epoch : 173 Test Loss  : 2.970        Test Acc 47.080       time: 1.662\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 174 Train Loss : 2.934         time: 35.230\n",
            "Epoch : 174 Test Loss  : 3.006        Test Acc 46.898       time: 1.654\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 175 Train Loss : 2.932         time: 35.223\n",
            "Epoch : 175 Test Loss  : 2.971        Test Acc 46.898       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 176 Train Loss : 2.931         time: 35.241\n",
            "Epoch : 176 Test Loss  : 2.966        Test Acc 47.628       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 177 Train Loss : 2.930         time: 35.240\n",
            "Epoch : 177 Test Loss  : 3.000        Test Acc 46.350       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 178 Train Loss : 2.912         time: 35.216\n",
            "Epoch : 178 Test Loss  : 2.960        Test Acc 50.365       time: 1.655\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 179 Train Loss : 2.902         time: 35.211\n",
            "Epoch : 179 Test Loss  : 2.947        Test Acc 50.182       time: 1.659\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 180 Train Loss : 2.900         time: 35.225\n",
            "Epoch : 180 Test Loss  : 2.965        Test Acc 50.182       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 181 Train Loss : 2.898         time: 35.243\n",
            "Epoch : 181 Test Loss  : 2.949        Test Acc 50.365       time: 1.656\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 182 Train Loss : 2.892         time: 35.231\n",
            "Epoch : 182 Test Loss  : 2.929        Test Acc 53.102       time: 1.667\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 183 Train Loss : 2.876         time: 35.263\n",
            "Epoch : 183 Test Loss  : 2.929        Test Acc 52.555       time: 1.665\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 184 Train Loss : 2.873         time: 35.278\n",
            "Epoch : 184 Test Loss  : 2.932        Test Acc 52.555       time: 1.673\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 185 Train Loss : 2.868         time: 35.261\n",
            "Epoch : 185 Test Loss  : 2.948        Test Acc 51.825       time: 1.657\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 186 Train Loss : 2.865         time: 35.220\n",
            "Epoch : 186 Test Loss  : 2.927        Test Acc 52.920       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 187 Train Loss : 2.863         time: 35.245\n",
            "Epoch : 187 Test Loss  : 2.923        Test Acc 53.102       time: 1.673\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 188 Train Loss : 2.863         time: 35.293\n",
            "Epoch : 188 Test Loss  : 2.922        Test Acc 52.920       time: 1.675\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 189 Train Loss : 2.861         time: 35.264\n",
            "Epoch : 189 Test Loss  : 2.923        Test Acc 52.920       time: 1.660\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 190 Train Loss : 2.851         time: 35.260\n",
            "Epoch : 190 Test Loss  : 2.913        Test Acc 54.562       time: 1.666\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 191 Train Loss : 2.847         time: 35.258\n",
            "Epoch : 191 Test Loss  : 2.910        Test Acc 54.562       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 192 Train Loss : 2.845         time: 35.248\n",
            "Epoch : 192 Test Loss  : 2.912        Test Acc 54.197       time: 1.663\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 193 Train Loss : 2.844         time: 35.240\n",
            "Epoch : 193 Test Loss  : 2.907        Test Acc 54.015       time: 1.670\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 194 Train Loss : 2.842         time: 35.257\n",
            "Epoch : 194 Test Loss  : 2.918        Test Acc 53.832       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 195 Train Loss : 2.842         time: 35.247\n",
            "Epoch : 195 Test Loss  : 2.925        Test Acc 54.562       time: 1.674\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 196 Train Loss : 2.843         time: 35.239\n",
            "Epoch : 196 Test Loss  : 2.901        Test Acc 54.197       time: 1.667\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 197 Train Loss : 2.844         time: 35.281\n",
            "Epoch : 197 Test Loss  : 2.943        Test Acc 53.285       time: 1.664\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 198 Train Loss : 2.841         time: 35.245\n",
            "Epoch : 198 Test Loss  : 2.918        Test Acc 54.562       time: 1.661\n",
            "--------------------------------------------------------------\n",
            "1.0000000000000004e-08\n",
            "Epoch : 199 Train Loss : 2.841         time: 35.262\n",
            "Epoch : 199 Test Loss  : 2.890        Test Acc 54.562       time: 1.680\n",
            "--------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JV9lO3-KXkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "c2ecfed9-0f90-48f1-9e5a-520dedd6d8f6"
      },
      "source": [
        "\n",
        "lossesTR\n",
        "dlossesTR[test_name]    = lossesTR\n",
        "dlossesTRAll[test_name] = lossesTRAll\n",
        "dlossesTE[test_name]    = lossesTE\n",
        "didxEpoch[test_name]    = idxEpoch\n",
        "dbestAcc[test_name]     = bestAcc\n",
        "dnbParam[test_name]     = count_parameters(net)\n",
        "\n",
        "#for n in ['convnet', 'CNN2']:\n",
        "for n in [ test_name ]: # ajouter dans cette liste le nom des reseaux que vous tester \n",
        "    if n in dlossesTR:\n",
        "        print ('----------------------------------------------------------------------------')\n",
        "        print ('----------------------------------------------------------------------------')\n",
        "        print ('----------------------------------------------------------------------------')\n",
        "        print (n)\n",
        "        print ('best accuracy      : '+str(dbestAcc[n].item()))\n",
        "        print ('best loss on train : '+str(np.min(dlossesTR[n])) + ' idx '+str(np.argmin(dlossesTR[n])))\n",
        "        print ('best loss on test  : '+str(np.min(dlossesTE[n])) + ' idx '+str(np.argmin(dlossesTE[n])))\n",
        "        print ('n param            : '+str(dnbParam[n]))\n",
        "\n",
        "        # evenly sampled time at 200ms intervals\n",
        "        t = np.arange(0, len(dlossesTRAll[n]))\n",
        "\n",
        "        plt.plot(t, dlossesTRAll[n], 'b', didxEpoch[n][1:], dlossesTR[n], 'ro', didxEpoch[n][1:], dlossesTE[n], 'gs')\n",
        "        plt.title(n)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------\n",
            "vgg_lr0_1.0e-05\n",
            "best accuracy      : 54.562042236328125\n",
            "best loss on train : 2.8409494468144008 idx 198\n",
            "best loss on test  : 2.8899988068474665 idx 199\n",
            "n param            : 39832566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ74HmTGgQyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b52d47-57ba-4d32-aa02-d95f610b058b"
      },
      "source": [
        " for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "   break\n",
        "print(batch_idx)\n",
        "output = net(inputs.cuda())\n",
        "print(output[0])\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor([3.3099e-04, 9.4652e-15, 3.1178e-13, 1.4330e-03, 2.8177e-04, 1.3383e-10,\n",
            "        3.3475e-14, 7.4578e-02, 2.1350e-14, 8.9435e-01, 7.8985e-14, 1.5551e-08,\n",
            "        6.9655e-08, 2.7980e-15, 6.0735e-15, 1.1134e-15, 2.1719e-15, 8.0750e-05,\n",
            "        9.7370e-07, 6.4735e-03, 5.0149e-03, 5.8757e-03, 2.6776e-03, 3.1460e-16,\n",
            "        5.2433e-15, 2.1321e-09, 1.4144e-11, 8.9050e-03, 4.1649e-14, 5.2521e-08],\n",
            "       device='cuda:0', grad_fn=<SelectBackward>)\n",
            "tensor([ 9, 14,  3,  0,  3, 20, 24, 20, 24, 18, 11, 26,  1, 16,  0, 18,  3,  2,\n",
            "        20,  3, 26, 20, 23, 21,  5, 17,  0, 29, 21,  7, 28, 24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2tkLasjHgh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}